{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install factor_analyzer\n",
    "# !pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
    "# conda install -c conda-forge scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "The primary dataframe we will be working with is df_fct, which is a dataframe composed of only the 36 factor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from customer_data file\n",
    "df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Create dataframe of factors only\n",
    "df_fct =  df.drop(['UID','Const'], axis=1)\n",
    "\n",
    "# Number of variables/factors/stimuli\n",
    "variables_to_examine = len(df_fct.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis\n",
    "\n",
    "The variables are all on a similar scale, so we will use the covariance matrix for identifying principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(n_factors=variables_to_examine, rotation=None)\n",
    "fa.fit(df_fct)\n",
    "\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "\n",
    "# Create FactorAnalyzer object\n",
    "n_factors = sum(i >= 1 for i in ev)\n",
    "fa = FactorAnalyzer(n_factors=n_factors, rotation=None)\n",
    "\n",
    "# Fit factor analysis model to variables\n",
    "fa.fit(df_fct)\n",
    "\n",
    "# Scores for the factor analysis, converted to dataframe\n",
    "scores = pd.DataFrame(fa.transform(df_fct))\n",
    "\n",
    "\n",
    "##############\n",
    "# Extra Stuff\n",
    "##############\n",
    "\n",
    "# The loadings are the coefficients that make up the linear combination of original variables to get the factors (v1 = l1x1 + l2X2)\n",
    "loadings = fa.loadings_\n",
    "\n",
    "# Create dataframe of eigenvalues of the covariance matrix\n",
    "\n",
    "data = {'factor'                    : range(1,n_factors+1),\n",
    "        'eigenvalues'               : fa.get_eigenvalues()[0][0:n_factors],\n",
    "        'common_factor_eigenvalues' : fa.get_eigenvalues()[1][0:n_factors],\n",
    "        'variance'                  : fa.get_factor_variance()[0],\n",
    "        'proportional_variance'     : fa.get_factor_variance()[1],\n",
    "        'cumulative_variance'       : fa.get_factor_variance()[2]\n",
    "       }\n",
    "\n",
    "cov_matrix_eigenvals = pd.DataFrame(data=data).set_index('factor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=[]\n",
    "\n",
    "for i in range(2,7):\n",
    "    \n",
    "    # Create clustering objects\n",
    "    cls1 = KMeans(n_clusters=i, random_state=0)\n",
    "    cls2 = KMedoids(n_clusters=i, random_state=0)\n",
    "    cls3 = AgglomerativeClustering(n_clusters=i, affinity = 'euclidean', linkage ='ward') #if linkage is ward, affinity must be Euclidean\n",
    "    cls_algs = [['kMeans', cls1], ['kMedoids', cls2], ['Hierarchical', cls3]]\n",
    "    \n",
    "    # Fit and score clustering solutions for i clusters with each clustering algorithm\n",
    "    for cls in cls_algs:\n",
    "        \n",
    "        # Fit the model to the factor analysis scores\n",
    "        cls[1].fit(scores)\n",
    "        \n",
    "        # List of assigned clusters\n",
    "        clusters = cls[1].fit_predict(scores)\n",
    "        \n",
    "        # Silhouette scores for each solution\n",
    "        silhouette_avg = silhouette_score(scores,clusters)\n",
    "        \n",
    "        # Store solution info [algorithm, number of clusters, avg silhouette score, cluster predictions]\n",
    "        algorithm = cls[0]\n",
    "        i_stats = [algorithm, i, silhouette_avg, clusters]\n",
    "        sw.append(i_stats)\n",
    "        \n",
    "        # Add columns of cluster assignments to df_fct datafram\n",
    "        df_fct[algorithm+'_'+'cluster'+'_'+str(i)] = clusters\n",
    "\n",
    "\n",
    "# Reorder cluster lists by descending silhouette scores.  Clusters in first element should be assigned to training data.\n",
    "sw = sorted(sw, key=itemgetter(2), reverse=True)\n",
    "\n",
    "# Add the labels to the training dataset (you can ignore the warning when the cell runs)\n",
    "df_fct['cluster'] = sw[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "#### Split into training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variable columns and the optimal cluster assignment\n",
    "data_of_interest = df_fct.iloc[:,np.r_[:variables_to_examine,-1]]\n",
    "\n",
    "# Split data into 75% training, 12.5% validation, 12.5% test\n",
    "train, valid = train_test_split(data_of_interest, test_size=0.25, random_state=123)\n",
    "\n",
    "valid, test = train_test_split(valid, test_size=0.5, random_state=123)\n",
    "\n",
    "# X is unlabeled training data, y is true training labels \n",
    "X, y = train.loc[:, train.columns != 'cluster'], train['cluster']\n",
    "\n",
    "X_valid, y_valid = valid.loc[:, train.columns != 'cluster'], valid['cluster']\n",
    "\n",
    "X_test, y_test = test.loc[:, test.columns != 'cluster'], test['cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_scores = []\n",
    "\n",
    "clf1 = RandomForestClassifier(random_state=0)\n",
    "clf2 = GradientBoostingClassifier(random_state=0)\n",
    "clf3 = SVC(random_state=0)\n",
    "clf4 = KNeighborsClassifier()\n",
    "\n",
    "classifiers = [['rf', clf1], ['gbt', clf2], ['svc', clf3], ['knn', clf4]]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    \n",
    "    # Fit classifier to training data\n",
    "    classifier[1].fit(X,y)    \n",
    "    \n",
    "    # Store classifier-specific results [algorithm object, classifier name, scores]\n",
    "    results = [classifier[1], classifier[0], classifier[1].score(X_valid,y_valid)]\n",
    "\n",
    "    # Overall classifier results\n",
    "    clf_scores.append(results)\n",
    "\n",
    "# Sort classifier accuracy in descending order\n",
    "clf_scores = sorted(clf_scores, key=itemgetter(1), reverse=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               avg  Relative Importance\n",
      "variable                               \n",
      "1         0.242242             1.000000\n",
      "2         0.080437             0.317447\n",
      "3         0.054669             0.208749\n",
      "4         0.047890             0.180152\n",
      "5         0.041253             0.152153\n",
      "6         0.040327             0.148246\n",
      "7         0.035441             0.127638\n",
      "8         0.030601             0.107218\n",
      "9         0.028794             0.099596\n",
      "10        0.025740             0.086714\n"
     ]
    }
   ],
   "source": [
    "# This should probably be a function that is nested within the classifier function\n",
    "\n",
    "importance = pd.DataFrame({'variable': list(range(1,37)),\n",
    "                           'rf': clf1.feature_importances_,\n",
    "                           'gbt': clf2.feature_importances_,\n",
    "                           'avg': (importance['rf']+importance['gbt'])/2},\n",
    "                         ).set_index('variable')\n",
    "\n",
    "importance['Relative Importance'] = np.interp(importance['avg'], (importance['avg'].min(), importance['avg'].max()), (0, 1))\n",
    "\n",
    "# View top 10 variables when RF and GBT models are averaged\n",
    "top_10_avg = importance.sort_values(by='avg', ascending=False)[['avg', 'Relative Importance']].head(10)\n",
    "\n",
    "print(top_10_avg)\n",
    "\n",
    "# 7 out of 10 of the top variables appear in both lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of the average importance of the difference variables\n",
    "# importance.sort_values(by='Relative Importance', ascending=False)['Relative Importance'].head(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_rank = list(range(1,37))\n",
    "importance = importance.sort_values(by='Relative Importance', ascending=False)\n",
    "importance['rank'] = importance_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing final accuracy of model with selected best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: 90.566% accuracy\n",
      "Binary variables: 73.585% accuracy\n"
     ]
    }
   ],
   "source": [
    "# Changes dataset into 0s and 1s depending on the value of the cell\n",
    "X_test_zero_one = X_test.mask(data_of_interest > 0, 1).mask(data_of_interest <= 0, 0)\n",
    "\n",
    "# Raw data accuracy = 0.9056603773584906\n",
    "print(f'Raw data: {round(clf_scores[0][0].score(X_test,y_test),5)*100}% accuracy')\n",
    "\n",
    "# Binary variables accuracy = 0.7358490566037735\n",
    "print(f'Binary variables: {round(clf_scores[0][0].score(X_test_zero_one,y_test),5)*100}% accuracy')\n",
    "\n",
    "# That seems like a pretty significant difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        42\n",
      "           1       0.75      0.82      0.78        11\n",
      "\n",
      "    accuracy                           0.91        53\n",
      "   macro avg       0.85      0.87      0.86        53\n",
      "weighted avg       0.91      0.91      0.91        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I don't think the necessarily needs to be in the final product, but it helps evaluate the models.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "valid_pred = clf3.predict(X_valid)\n",
    "\n",
    "print(classification_report(y_valid, valid_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
